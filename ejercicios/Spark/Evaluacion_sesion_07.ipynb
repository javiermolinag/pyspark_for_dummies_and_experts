{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38577af2-d648-4b03-82b5-f44adadae9b0",
   "metadata": {},
   "source": [
    "#### El cliente nos ha solicitado realizar algunos métodos que resuelvan consultas específicas sobre las tablas movies_df, ratings_df y tags_df. Lee cuidadosamente cada consulta y desarrolla el método correspondiente dada la firma del método requerida.\n",
    "\n",
    "##### Nota: Para poder trabajar con este notebook es necesario haber terminado el ejercicio de la sesión 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2ac7f-a13f-495e-85d4-29efca6bfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML <style>pre { white-space: pre !important; }</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5e3c8-5689-4c7c-83d3-ec7b70d0e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame, Column, Row}\n",
    "import org.apache.spark.sql.{functions => f}\n",
    "import org.apache.spark.sql.{types => t}\n",
    "\n",
    "def difference(l1: Seq[String], l2: Seq[String]): Seq[Column] =\n",
    "    l1.diff(l2).map(colName => f.col(colName))\n",
    "\n",
    "def readTmpDf(dfSeq: Seq[String]): Map[String, DataFrame] =\n",
    "    dfSeq.map(table_name => (table_name, spark.read.parquet(\"../../resources/data/tmp/parquet/\" + table_name))).toMap\n",
    "\n",
    "def writeTmpDf(dfSeq: Seq[(DataFrame, String)]): Unit = \n",
    "    dfSeq.foreach{case (df: DataFrame, name: String) => df.write.mode(\"overwrite\").parquet(\"../../resources/data/tmp/parquet/\" + name)}\n",
    "\n",
    "def schema_to_ddl(df: DataFrame): String = df.schema.toDDL.replace(\" NOT NULL\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f34787-67c5-4d2b-a592-4f29e250e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR CONTENIDO DE ESTA CELDA\n",
    "\n",
    "// Creación de sesión de Spark\n",
    "val spark = SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"ejercicio_7\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"GMT-6\")\n",
    "\n",
    "// Carga de tablas requeridas\n",
    "val RootPath = \"../../resources/data/tmp/parquet/\"\n",
    "val namesList = Seq(\"06/movies\", \"06/ratings\", \"06/tags\")\n",
    "val dfMap = readTmpDf(namesList)\n",
    "\n",
    "val moviesDf = dfMap(\"06/movies\")\n",
    "val ratingsDf = dfMap(\"06/ratings\")\n",
    "val tagsDf = dfMap(\"06/tags\")\n",
    "\n",
    "moviesDf.show(1, false)\n",
    "ratingsDf.show(1)\n",
    "tagsDf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c8e25-807a-4e35-b763-57dcab283721",
   "metadata": {},
   "source": [
    "#### Actividad 1:\n",
    "##### TO DO ->    Para el dataframe \"moviesDf\":\n",
    "- ##### Genera un método llamado getAllGenres que retorne un DataFrame con únicamente una columna conteniendo todos los valores distintos (sin repetir) de la columna \"genres\"\n",
    "    - ##### Firma: def getAllGenres(df: DataFrame): DataFrame\n",
    "    - Apoyate de la funcion explode -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html#pyspark.sql.functions.explode\n",
    "\n",
    "- ##### Genera un método llamado get_min_year que retorne un valor de tipo int que contenga el menor año registrado (omitiendo nulls)\n",
    "    - ##### Firma: def getMinYear(df: DataFrame): Int\n",
    "    - ##### Necesitarás llamar alguna de las siguientes acciones: take, first, head.\n",
    "\n",
    "- ##### Genera un método llamado get_min_year que retorne un valor de tipo int que contenga el mayor año registrado (omitiendo nulls)\n",
    "    - ##### Firma: def getMaxYear(df: DataFrame): Int\n",
    "    - ##### Necesitarás llamar alguna de las siguientes acciones: take, first, head.\n",
    "\n",
    "##### NO UTILIZAR withColumn NI withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef84506-2869-4e5d-9d5e-3104a30f29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "// TU CODIGO VA EN ESTA CELDA:\n",
    "\n",
    "def getAllGenres(df: DataFrame): DataFrame =\n",
    "    df // modificar codigo interno\n",
    "    \n",
    "def getMinYear(df: DataFrame): Int =\n",
    "    return df.select(\"year\").first.getAs[Int](\"year\") // modificar codigo interno\n",
    "    \n",
    "def getMaxYear(df: DataFrame): Int =\n",
    "    return df.select(\"year\").first.getAs[Int](\"year\") // modificar codigo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68252b23-7945-4b65-9994-20980eb23889",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "\n",
    "getAllGenres(moviesDf).show(20, false)\n",
    "\"\"\"\n",
    "Ejemplo de salida esperada (el nombre de la columna podría ser distinto):\n",
    "+-----------+\n",
    "|col        |\n",
    "+-----------+\n",
    "|Crime      |\n",
    "|Romance    |\n",
    "|Thriller   |\n",
    "|Adventure  |\n",
    "|Drama      |\n",
    "|War        |\n",
    "|Documentary|\n",
    "|Fantasy    |\n",
    "|Mystery    |\n",
    "|Musical    |\n",
    "|Animation  |\n",
    "|Film-Noir  |\n",
    "|IMAX       |\n",
    "|Horror     |\n",
    "|Western    |\n",
    "|Comedy     |\n",
    "|Children   |\n",
    "|Action     |\n",
    "|Sci-Fi     |\n",
    "+-----------+\n",
    "\"\"\"\n",
    "println(getMinYear(moviesDf))\n",
    "// Salida esperada: 1874\n",
    "println(getMaxYear(moviesDf))\n",
    "// Salida esperada: 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe845e-3491-4366-8759-49d8eae1b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "\n",
    "val genresDf = getAllGenres(moviesDf)\n",
    "\n",
    "assert(genresDf.isInstanceOf[DataFrame])\n",
    "assert(genresDf.columns.size == 1)\n",
    "\n",
    "val expectedOutput = Seq(\"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\", \n",
    "                         \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \n",
    "                         \"Horror\", \"IMAX\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \n",
    "                         \"Thriller\", \"War\", \"Western\")\n",
    "val genresList = genresDf.rdd.map(item => item(0).toString).collect().sortWith(_.compareTo(_) < 0)\n",
    "assert(genresList.diff(expectedOutput).size + expectedOutput.diff(genresList).size == 0)\n",
    "\n",
    "val minYear = getMinYear(moviesDf)\n",
    "assert(minYear == 1874)\n",
    "\n",
    "val maxYear = getMaxYear(moviesDf)\n",
    "assert(maxYear == 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1377e4-71a4-4479-bc6c-a7b6288164d0",
   "metadata": {},
   "source": [
    "#### Actividad 2:\n",
    "##### TO DO -> Para la tabla \"ratingsDf\" el cliente requiere hacer un análisis de cada \"movie_id\" para tener una idea de qué tan buena es cada pelicula, asi que nos solicitó desarrollar un método que realice múltiples cálculos.\n",
    "##### Generar un método que retorne un DataFrame con las columnas especificadas por cada \"movie_id\":\n",
    "- ##### Firma: def calculateRatingValues(df: DataFrame): DataFrame\n",
    "- ##### Columnas generadas:\n",
    "    - ##### nombre: avg_rating, tipo: DoubleType() -> Valor rating promedio (Redondear a 2 decimales)\n",
    "    - ##### nombre: stddev_rating, tipo: DoubleType() -> Desviacion estándar (stddev_pop) para la columna rating (Redondear a 2 decimales)\n",
    "        - Para redondear valores utiliza la función round de Spark -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.round.html#pyspark.sql.functions.round\n",
    "    - ##### nombre: count_rating, tipo: LongType() -> Total de calificaciones recibidas\n",
    "- ##### ACTUALIZACIÓN: Para aquellas peliculas (movie_id) que no se tenga identificado su año en la tabla \"movies_df\" nos han solicitado calcular el año de la siguiente manera (más adelante nos preocuparemos por pegar el año a la tabla movies_df):\n",
    "    - ##### nombre: min_time_rating, tipo: TimestampType() -> Fecha más antigua de la columna \"time\" en la que se asignó el primer rating\n",
    "##### Nota 1: podemos hacer el calculo de \"min_time_rating\" en la misma transformación en las que se generan las columnas \"avg_rating\", \"stddev_rating\", y \"count_rating\"\n",
    "##### Nota 2: Posiblemente requieras analizar las funciones de agregación existentes en Spark -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#aggregate-functions\n",
    "* El dataframe de salida deberá contar con la siguiente estructura al hacer printSchema():\n",
    "* |-- movie_id: string\n",
    "* |-- avg_rating: double\n",
    "* |-- stddev_rating: double\n",
    "* |-- count_rating: long\n",
    "* |-- min_time_rating: timestamp\n",
    "##### NO UTILIZAR withColumn NI withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050433f-5e9d-459f-baa5-eb289a05f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// TU CODIGO VA EN ESTA CELDA:\n",
    "\n",
    "def calculateRatingValues(df: DataFrame): DataFrame = \n",
    "    df //    modificar codigo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac13b5-d99c-468b-80ce-908808d21a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "calculateRatingValues(ratingsDf).show(2)\n",
    "\"\"\"\n",
    "Ejemplo de salida esperada:\n",
    "+--------+----------+-------------+------------+-------------------+\n",
    "|movie_id|avg_rating|stddev_rating|count_rating|    min_time_rating|\n",
    "+--------+----------+-------------+------------+-------------------+\n",
    "|     296|      4.19|         0.95|      108756|1996-02-29 10:48:44|\n",
    "|  115713|      3.99|         0.83|       21335|2015-01-02 06:05:51|\n",
    "+--------+----------+-------------+------------+-------------------+\n",
    "only showing top 2 rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66937ba3-9f1a-42b2-801f-4b5daf250796",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "import java.sql.Timestamp\n",
    "\n",
    "val ratingValuesDf = calculateRatingValues(ratingsDf)\n",
    "\n",
    "assert(ratingValuesDf.isInstanceOf[DataFrame])\n",
    "assert(ratingValuesDf.columns.size == 5)\n",
    "assert(ratingValuesDf.count() == 83239)\n",
    "\n",
    "val data = Seq(Row(\"296\", 4.19, 0.95, 108756L, Timestamp.valueOf(\"1996-02-29 10:48:44.0\")))\n",
    "\n",
    "val schema = t.StructType(Seq(\n",
    "    t.StructField(\"movie_id\", t.StringType),\n",
    "    t.StructField(\"avg_rating\", t.DoubleType),\n",
    "    t.StructField(\"stddev_rating\", t.DoubleType),\n",
    "    t.StructField(\"count_rating\", t.LongType),\n",
    "    t.StructField(\"min_time_rating\", t.TimestampType)))\n",
    "\n",
    "val testDf = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n",
    "assert(ratingValuesDf.filter(f.col(\"movie_id\") === \"296\").except(testDf).count() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fba01-1cfc-4cad-906c-760eb205745d",
   "metadata": {},
   "source": [
    "#### Actividad 3:\n",
    "##### TO DO -> El cliente ha solicitado generar dos tablas con el mismo contenido pero con distinto esquema a partir de la tabla \"tagsDf\", lee a continuación la información enviada con los requerimientos:\n",
    "- ##### En ambas tablas se requiere agrupar por \"movie_id\" y calcular:\n",
    "    - ##### El total de veces en las que aparece cada tag (en mayusculas) .\n",
    "    - ##### Fecha más antigua en la que se asignó el primer tag.\n",
    "La primer tabla deberá tener la siguiente estructura:  \n",
    "*      | movie_id |             tag_count |        min_time_tag |\n",
    "*      |        1 | [SCI-FI:2, TERROR:12] | 2017-06-02 07:20:27 |\n",
    "*      |        3 |  [DRAMA:14, SCI-FI:4] | 2012-06-02 07:20:27 |\n",
    "con esquema:\n",
    "*      |-- movie_id: string\n",
    "*      |-- tag_count: array\n",
    "*      |    |-- element: string\n",
    "*      |-- min_time_tag: timestamp\n",
    "La segunda tabla deberá tener la siguiente estructura:\n",
    "*      | movie_id |                   tag_count |        min_time_tag |\n",
    "*      |        1 | [{SCI-FI, 2}, {TERROR, 12}] | 2017-06-02 07:20:27 |\n",
    "*      |        3 |  [{DRAMA, 14}, {SCI-FI, 4}] | 2012-06-02 07:20:27 |\n",
    "con esquema:\n",
    "*      |-- movie_id: string\n",
    "*      |-- tag_count: array\n",
    "*      |    |-- element: struct\n",
    "*      |    |    |-- tag: string\n",
    "*      |    |    |-- count: long\n",
    "*      |-- min_time_tag: timestamp\n",
    "- ##### NOTA: Ordena la columna tag_count, la cual es un array, de forma ascendente.\n",
    "- #### La generación de la primer tabla es a través del método con la firma:\n",
    "    - ##### def getAct3Df1(df: DataFrame): DataFrame\n",
    "- #### La generación de la segunda tabla es a través del método con la firma:\n",
    "    - ##### def getAct3Df2(df: DataFrame): DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a99bc8-bf57-47c3-95c7-4e67e013087f",
   "metadata": {},
   "source": [
    "#### La siguiente información muestra los pasos recomendados para resolver la actividad 3, puedes omitir leer esta parte si asi lo consideras.\n",
    "##### Para generar ambas tablas primero necesitamos obtener el total veces (count) en las que un \"tag\" se repite por cada \"movie_id\", para evitar conteos erroneos hay que convertir cada tag en Mayusculas.\n",
    "##### El dataframe resultante deberá contener la siguiente estructura: \"movie_id\", \"tag\", \"count\" y \"min\" donde la columna \"count\" representa el total de veces que aparecen cada \"tag\"y \"movie_id\"; la columna \"min\" representa el valor mínimo de cada \"tag\" y \"movie_id\".\n",
    "Por ejemplo, dado el dataframe de entrada\n",
    "*      |user_id|movie_id|   tag|               time|\n",
    "*      |    183|     100|sci-fi|2012-06-02 07:20:27|\n",
    "*      |     12|     832| Drama|2017-06-01 07:20:27|\n",
    "*      |    251|     100|SCI-FI|2009-06-04 07:20:27|\n",
    "*      |    265|     832| DRAMA|2015-06-08 07:20:27|\n",
    "*      |     22|     100|terror|2020-06-06 07:20:27|\n",
    "\n",
    "debemos obtener el siguiente dataframe (el nombre de columnas \"count\" y \"min\" podria ser distinto):\n",
    "\n",
    "*      | movie_id |    tag | count |                 min |\n",
    "*      |      100 | TERROR |     1 | 2020-06-06 07:20:27 |\n",
    "*      |      100 | SCI-FI |     2 | 2009-06-04 07:20:27 |\n",
    "*      |      832 |  DRAMA |     2 | 2015-06-08 07:20:27 |\n",
    "\n",
    "##### Para generar la primer tabla necesitamos concatenar las columnas \"tag\" y \"count\", posteriormente agrupando por \"movie_id\" generaremos la lista de elementos requerida. La columna \"min_time_tag\" representa el valor mínimo de cada \"movie_id\".\n",
    "- ##### Funciones de Spark recomendadas:\n",
    "    - upper -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html#pyspark.sql.functions.upper\n",
    "    - concat_ws -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat_ws.html#pyspark.sql.functions.concat_ws\n",
    "    - concat -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat.html#pyspark.sql.functions.concat\n",
    "    - collect_list -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list\n",
    "    - collect_set -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set##### \n",
    "    - sort_array -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sort_array.html#pyspark-sql-functions-sort-array\n",
    "##### La primer tabla quedará estructurada de la siguiente manera, donde la columna \"tag_count\" es de tipo ArrayType(StringType()) y la columna \"min_time_tag\" es de tipo TimestampType():\n",
    "Dado el dataframe de entrada\n",
    "*      | movie_id |    tag | count |                 min |\n",
    "*      |        1 | TERROR |    12 | 2020-06-06 07:20:27 |\n",
    "*      |        1 | SCI-FI |     2 | 2015-06-06 07:20:27 |\n",
    "*      |        3 |  DRAMA |    14 | 2004-06-06 07:20:27 |\n",
    "*      |        3 | SCI-FI |     4 | 2012-06-06 07:20:27 |\n",
    "debemos obtener el siguiente dataframe\n",
    "*       | movie_id |             tag_count |        min_time_tag |\n",
    "*       |        1 | [SCI-FI:2, TERROR:12] | 2015-06-06 07:20:27 |\n",
    "*       |        3 |  [DRAMA:14, SCI-FI:4] | 2004-06-06 07:20:27 |\n",
    "##### Para generar la segunda tabla necesitamos agregar en una estructura las columnas \"tag\" y \"count\", posteriormente agrupando por movie_id generaremos la lista de elementos requerida. La columna \"min_time_tag\" representa el valor mínimo de cada \"movie_id\".\n",
    "- ##### Funciones de Spark recomendadas:\n",
    "    - upper -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html#pyspark.sql.functions.upper\n",
    "    - struct -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.struct.html#pyspark.sql.functions.struct\n",
    "    - collect_list -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list\n",
    "    - collect_set -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set\n",
    "    - sort_array -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.sort_array.html#pyspark-sql-functions-sort-array\n",
    "##### La segunda tabla quedará estructurada de la siguiente manera, donde la columna \"tag_count\" de tipo ArrayType(StructType(StringType(),LongType()))  y la columna \"min_time_tag\" es de tipo TimestampType():\n",
    "Dado el dataframe de entrada\n",
    "*      | movie_id |    tag | count |                 min |\n",
    "*      |        1 | TERROR |    12 | 2020-06-06 07:20:27 |\n",
    "*      |        1 | SCI-FI |     2 | 2015-06-06 07:20:27 |\n",
    "*      |        3 |  DRAMA |    14 | 2004-06-06 07:20:27 |\n",
    "*      |        3 | SCI-FI |     4 | 2012-06-06 07:20:27 |\n",
    "debemos obtener el siguiente dataframe\n",
    "*      | movie_id |                   tag_count |        min_time_tag |\n",
    "*      |        1 | [{SCI-FI, 2}, {TERROR, 12}] | 2015-06-06 07:20:27 |\n",
    "*      |        3 |  [{DRAMA, 14}, {SCI-FI, 4}] | 2004-06-06 07:20:27 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e5d81-6c6a-4e23-ad26-2da273a001e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// TU CODIGO VA EN ESTA CELDA, PUEDES GENERAR MÉTODOS O VARIABLES NUEVAS SI ASI LO REQUIERES\n",
    "\n",
    "def getAct3Df1(df: DataFrame): DataFrame =\n",
    "    df // ... transformaciones a tagsDf\n",
    "\n",
    "def getAct3Df2(df: DataFrame): DataFrame =\n",
    "    df // ... transformaciones a tagsDf\n",
    "\n",
    "\n",
    "val act3Df1: DataFrame = getAct3Df1(tagsDf)\n",
    "val act3Df2: DataFrame = getAct3Df2(tagsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45e9bc-cd80-4e9f-9024-613aaf1f63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "act3Df1.show(2, false)\n",
    "act3Df2.show(2, false)\n",
    "\"\"\"\n",
    "Ejemplo de salida esperada:\n",
    "+--------+------------------------------------------------------------------+-------------------+\n",
    "|movie_id|tag_count                                                         |min_time_tag       |\n",
    "+--------+------------------------------------------------------------------+-------------------+\n",
    "|100062  |[FATE : 1, PRESS-GANGED : 1, WAR : 1, WORLD WAR II : 1]           |2018-05-26 16:40:54|\n",
    "|100070  |[COMEDIAN : 2, COMEDY : 1, GOOD HUMOUR : 1, STRUGGLING CAREER : 1]|2017-05-19 17:17:36|\n",
    "+--------+------------------------------------------------------------------+-------------------+\n",
    "only showing top 2 rows\n",
    "\n",
    "+--------+----------------------------------------------------------------------+-------------------+\n",
    "|movie_id|tag_count                                                             |min_time_tag       |\n",
    "+--------+----------------------------------------------------------------------+-------------------+\n",
    "|100062  |[{FATE, 1}, {PRESS-GANGED, 1}, {WAR, 1}, {WORLD WAR II, 1}]           |2018-05-26 16:40:54|\n",
    "|100070  |[{COMEDIAN, 2}, {COMEDY, 1}, {GOOD HUMOUR, 1}, {STRUGGLING CAREER, 1}]|2017-05-19 17:17:36|\n",
    "+--------+----------------------------------------------------------------------+-------------------+\n",
    "only showing top 2 rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ead994-12cb-46be-a2a6-16eb5eeb9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "import java.sql.Timestamp\n",
    "\n",
    "val expectedValueDf1 = Seq(Row(\"100070\",\n",
    "                               List(\"COMEDIAN : 2\",\n",
    "                                    \"COMEDY : 1\",\n",
    "                                    \"GOOD HUMOUR : 1\",\n",
    "                                    \"STRUGGLING CAREER : 1\"),\n",
    "                               Timestamp.valueOf(\"2017-05-19 17:17:36.0\")))\n",
    "val schemaDf1 = t.StructType(Seq(\n",
    "    t.StructField(\"movie_id\", t.StringType),\n",
    "    t.StructField(\"tag_count\", t.ArrayType(t.StringType)),\n",
    "    t.StructField(\"min_time_tag\", t.TimestampType)\n",
    "))\n",
    "assert(act3Df1.columns.size == 3)\n",
    "assert(act3Df1.columns.toSeq.contains(\"movie_id\"))\n",
    "assert(act3Df1.columns.toSeq.contains(\"tag_count\"))\n",
    "assert(act3Df1.columns.toSeq.contains(\"min_time_tag\"))\n",
    "assert(schema_to_ddl(act3Df1.select(\"movie_id\", \"tag_count\", \"min_time_tag\")) == \"movie_id STRING,tag_count ARRAY<STRING>,min_time_tag TIMESTAMP\")\n",
    "assert(act3Df1.count() == 53452)\n",
    "val testDf1 = spark.createDataFrame(spark.sparkContext.parallelize(expectedValueDf1), schemaDf1)\n",
    "assert(act3Df1.select(\"movie_id\", \"tag_count\", \"min_time_tag\").filter(f.col(\"movie_id\") === \"100070\").except(testDf1).count() == 0)\n",
    "\n",
    "val expectedValueDf2 = Seq(Row(\"100070\",\n",
    "                               List(Row(\"COMEDIAN\",2L),\n",
    "                                    Row(\"COMEDY\",1L),\n",
    "                                    Row(\"GOOD HUMOUR\",1L),\n",
    "                                    Row(\"STRUGGLING CAREER\",1L)),\n",
    "                               Timestamp.valueOf(\"2017-05-19 17:17:36.0\")))\n",
    "val schemaDf2 = t.StructType(Seq(\n",
    "    t.StructField(\"movie_id\", t.StringType),\n",
    "    t.StructField(\"tag_count\", t.ArrayType(t.StructType(Seq(\n",
    "        t.StructField(\"tag\", t.StringType),\n",
    "        t.StructField(\"count\", t.LongType)\n",
    "    )))),\n",
    "    t.StructField(\"min_time_tag\", t.TimestampType)\n",
    "))\n",
    "assert(act3Df2.columns.size == 3)\n",
    "assert(act3Df2.columns.toSeq.contains(\"movie_id\"))\n",
    "assert(act3Df2.columns.toSeq.contains(\"tag_count\"))\n",
    "assert(act3Df2.columns.toSeq.contains(\"min_time_tag\"))\n",
    "assert(schema_to_ddl(act3Df2.select(\"movie_id\", \"tag_count\", \"min_time_tag\")) == \"movie_id STRING,tag_count ARRAY<STRUCT<tag: STRING, count: BIGINT>>,min_time_tag TIMESTAMP\")\n",
    "assert(act3Df2.count() == 53452)\n",
    "val testDf2 = spark.createDataFrame(spark.sparkContext.parallelize(expectedValueDf2), schemaDf2)\n",
    "assert(act3Df2.select(\"movie_id\", \"tag_count\", \"min_time_tag\").filter(f.col(\"movie_id\") === \"100070\").except(testDf2).count() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a5e31-597f-4117-b460-df940f85f2da",
   "metadata": {},
   "source": [
    "#### Actividad 4:\n",
    "##### TO DO -> El cliente ha solicitado que resolvamos dos consultas con la salida de la actividad 3, como no especificó con cuál dataframe requiere la consulta lo realizaremos con ambos dataframes (act3Df1 y act3Df2).\n",
    "##### Las consultas a resolver son:\n",
    "- ##### 1.- ¿Cuál es la pelicula (movie_id) con más tags con el valor \"SCI-FI\"?\n",
    "    - ##### Estrictamente especificó no tomar en cuenta datos como \"REALISTIC SCI-FI\", \"HARD SCI-FI\", etc.\n",
    "- ##### 2.- ¿Cuántas peliculas fueron etiquetadas como \"SCI-FI\"?\n",
    "##### La forma de resolver estas consultas será a través de un método el cual va a retornar una tupla (str, int), donde el primer elemento (str) representa el resultado de la consulta 1 y el segundo elemento (int) representa el resultado de la consulta 2\n",
    "- ##### La firma del método que utilizará cada dataFrame es:\n",
    "    - ##### def exercise4Df1(df: DataFrame): (String, Long) -> firma para el dataFrame act3Df1\n",
    "    - ##### def exercise4Df2(df: DataFrame): (String, Long) -> firma para el dataFrame act3Df2\n",
    "##### Funciones de Spark recomendadas:\n",
    "- explode -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html#pyspark-sql-functions-explode\n",
    "- regexp_extract -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_extract.html#pyspark.sql.functions.regexp_extract\n",
    "##### Funciones de la clase Column recomendadas:\n",
    "- like -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.like.html#pyspark.sql.Column.like\n",
    "- ilike -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.ilike.html#pyspark.sql.Column.ilike\n",
    "- getField -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.getField.html#pyspark.sql.Column.getField\n",
    "##### Acciones que podrias utilizar:\n",
    "- count, first, head, take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8701f0-c9e5-4561-bafe-97346f3552d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "// TU CODIGO VA EN ESTA CELDA, PUEDES GENERAR MÉTODOS O VARIABLES SI ASI LO REQUIERES\n",
    "\n",
    "def exercise4Df1(df: DataFrame): (String, Long) =\n",
    "    (df.first.getAs[String](\"movie_id\"), df.count()) // modificar codigo interno\n",
    "\n",
    "def exercise4Df2(df: DataFrame): (String, Long) =\n",
    "    (df.first.getAs[String](\"movie_id\"), df.count()) // modificar codigo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8378f9d-5823-43db-b8cf-86c3cdf99e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "println(exercise4Df1(act3Df1))\n",
    "println(exercise4Df2(act3Df2))\n",
    "// Salida esperada en ambos casos\n",
    "//('260', 854)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df8125-9055-445b-8afe-aa8d123b8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "val resultExercise4Df1 = exercise4Df1(act3Df1)\n",
    "assert(resultExercise4Df1._1 == \"260\")\n",
    "assert(resultExercise4Df1._2 == 854)\n",
    "\n",
    "val resultExercise4Df2 = exercise4Df2(act3Df2)\n",
    "assert(resultExercise4Df2._1 == \"260\")\n",
    "assert(resultExercise4Df2._2 == 854)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f87e6-bdcf-4567-a8fe-56d4f1aa36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "// NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "val dfs = Seq((moviesDf, \"07/movies\"),\n",
    "              (act3Df1, \"07/tags_p1\"),\n",
    "              (act3Df2, \"07/tags_p2\"),\n",
    "              (calculateRatingValues(ratingsDf), \"07/ratings\"))\n",
    "\n",
    "writeTmpDf(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76900e-7ba9-4d99-bb20-2540cca7d4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
