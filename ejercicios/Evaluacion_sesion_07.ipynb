{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38577af2-d648-4b03-82b5-f44adadae9b0",
   "metadata": {},
   "source": [
    "#### El cliente nos ha solicitado realizar algunos métodos que resuelvan consultas específicas sobre laS tablaS movies_df, ratings_df y tags_df. Lee cuidadosamente cada consulta y desarrolla el método correspondiente dada la firma del método requerida.\n",
    "\n",
    "##### Nota: Para poder trabajar con este notebook es necesario haber terminado el ejercicio de la sesión 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5e3c8-5689-4c7c-83d3-ec7b70d0e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f34787-67c5-4d2b-a592-4f29e250e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "# Creación de sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ejercicio_7\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carga de tablas requeridas\n",
    "root_path = \"../resources/data/tmp/parquet/\"\n",
    "names_list = [\"06/movies\", \"06/ratings\", \"06/tags\"]\n",
    "df_dict = read_tmp_df(names_list)\n",
    "\n",
    "movies_df = df_dict[\"06/movies\"]\n",
    "ratings_df = df_dict[\"06/ratings\"]\n",
    "tags_df = df_dict[\"06/tags\"]\n",
    "\n",
    "movies_df.show(1, False)\n",
    "ratings_df.show(1)\n",
    "tags_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c8e25-807a-4e35-b763-57dcab283721",
   "metadata": {},
   "source": [
    "#### Actividad 1:\n",
    "##### TO DO ->    Para el dataframe \"movies_df\":\n",
    "- ##### Genera un método llamado get_all_genres que retorne un DataFrame con únicamente una columna conteniendo todos los valores distintos (sin repetir) de la columna \"genres\"\n",
    "    - ##### Firma: def get_all_genres(df: DataFrame) -> DataFrame\n",
    "    - Apoyate de la funcion explode -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html#pyspark.sql.functions.explode\n",
    "\n",
    "- ##### Genera un método llamado get_min_year que retorne un valor de tipo int que contenga el menor año registrado (omitiendo nulls)\n",
    "    - ##### Firma: def get_min_year(df: DataFrame) -> int\n",
    "    - ##### Necesitarás llamar alguna de las siguientes acciones: take, first, head.\n",
    "\n",
    "- ##### Genera un método llamado get_min_year que retorne un valor de tipo int que contenga el mayor año registrado (omitiendo nulls)\n",
    "    - ##### Firma: def get_max_year(df: DataFrame) -> int\n",
    "    - ##### Necesitarás llamar alguna de las siguientes acciones: take, first, head.\n",
    "\n",
    "##### NO UTILIZAR withColumn NI withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef84506-2869-4e5d-9d5e-3104a30f29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "\n",
    "def get_all_genres(df: DataFrame) -> DataFrame:\n",
    "    return df # modificar codigo interno\n",
    "    \n",
    "def get_min_year(df: DataFrame) -> int\n",
    "    return df.select(\"year\").first()[0] # modificar codigo interno\n",
    "    \n",
    "def get_max_year(df: DataFrame) -> int\n",
    "    return df.select(\"year\").first()[0] # modificar codigo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68252b23-7945-4b65-9994-20980eb23889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "\n",
    "get_all_genres(movies_df).show(20, False)\n",
    "\"\"\"\n",
    "Ejemplo de salida esperada (el nombre de la columna podría ser distinto):\n",
    "+-----------+\n",
    "|col        |\n",
    "+-----------+\n",
    "|Crime      |\n",
    "|Romance    |\n",
    "|Thriller   |\n",
    "|Adventure  |\n",
    "|Drama      |\n",
    "|War        |\n",
    "|Documentary|\n",
    "|Fantasy    |\n",
    "|Mystery    |\n",
    "|Musical    |\n",
    "|Animation  |\n",
    "|Film-Noir  |\n",
    "|IMAX       |\n",
    "|Horror     |\n",
    "|Western    |\n",
    "|Comedy     |\n",
    "|Children   |\n",
    "|Action     |\n",
    "|Sci-Fi     |\n",
    "+-----------+\n",
    "\"\"\"\n",
    "print(get_min_year(movies_df))\n",
    "# Salida esperada: 1874\n",
    "print(get_max_year(movies_df))\n",
    "# Salida esperada: 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe845e-3491-4366-8759-49d8eae1b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "\n",
    "genres_df = get_all_genres(movies_df)\n",
    "\n",
    "assert type(genres_df) == DataFrame\n",
    "assert len(genres_df.columns) == 1\n",
    "\n",
    "genres_list = genres_df.rdd.map(lambda item: item[0]).collect()\n",
    "genres_list.sort()\n",
    "assert genres_list == ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', \n",
    "                       'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', \n",
    "                       'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', \n",
    "                       'Thriller', 'War', 'Western']\n",
    "\n",
    "min_year = get_min_year(movies_df)\n",
    "assert min_year == 1874\n",
    "\n",
    "max_year = get_max_year(movies_df)\n",
    "assert max_year == 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1377e4-71a4-4479-bc6c-a7b6288164d0",
   "metadata": {},
   "source": [
    "#### Actividad 2:\n",
    "##### TO DO -> Para la tabla \"ratings_df\" el cliente requiere hacer un análisis de cada \"movie_id\" para tener una idea de qué tan buena es cada pelicula, asi que nos solicitó desarrollar un método que realice múltiples cálculos.\n",
    "##### Generar un método que retorne un DataFrame con las columnas especificadas por cada \"movie_id\":\n",
    "- ##### Firma: def calculate_rating_values(df: DataFrame) -> DataFrame\n",
    "- ##### Columnas generadas:\n",
    "    - ##### nombre: avg_rating, tipo: DoubleType() -> Valor rating promedio (Redondear a 2 decimales)\n",
    "    - ##### nombre: stddev_rating, tipo: DoubleType() -> Desviacion estándar para la columna rating (Redondear a 2 decimales)\n",
    "        - Para redondear valores utiliza la función round de Spark -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.round.html#pyspark.sql.functions.round\n",
    "    - ##### nombre: count_rating, tipo: LongType() -> Total de calificaciones recibidas\n",
    "- ##### ACTUALIZACIÓN: Para aquellas peliculas (movie_id) que no se tenga identificado su año en la tabla \"movies_df\" nos han solicitado calcular el año de la siguiente manera:\n",
    "    - ##### nombre: min_time_rating, tipo: TimestampType() -> Fecha más antigua de la columna \"time\" en la que se asignó el primer rating\n",
    "##### Nota 1: podemos hacer el calculo de \"min_time_rating\" en la misma transformación en las que se generan las columnas \"avg_rating\", \"stddev_rating\", y \"count_rating\"\n",
    "##### Nota 2: Posiblemente requieras analizar las funciones de agregación existentes en Spark -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#aggregate-functions\n",
    "* El dataframe de salida deberá contar con la siguiente estructura al hacer printSchema():\n",
    "* |-- movie_id: string\n",
    "* |-- avg_rating: double\n",
    "* |-- stddev_rating: double\n",
    "* |-- count_rating: long\n",
    "* |-- min_time_rating: timestamp\n",
    "##### NO UTILIZAR withColumn NI withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050433f-5e9d-459f-baa5-eb289a05f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA:\n",
    "\n",
    "def calculate_rating_values(df: DataFrame) -> DataFrame\n",
    "    return df # modificar codigo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac13b5-d99c-468b-80ce-908808d21a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "calculate_rating_values(ratings_df).show(2)\n",
    "\"\"\"\n",
    "Ejemplo de salida esperada:\n",
    "+--------+----------+-------------+------------+-------------------+\n",
    "|movie_id|avg_rating|stddev_rating|count_rating|    min_time_rating|\n",
    "+--------+----------+-------------+------------+-------------------+\n",
    "|     296|      4.19|         0.95|      108756|1996-02-29 10:48:44|\n",
    "|  115713|      3.99|         0.83|       21335|2015-01-02 06:05:51|\n",
    "+--------+----------+-------------+------------+-------------------+\n",
    "only showing top 2 rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66937ba3-9f1a-42b2-801f-4b5daf250796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "\n",
    "rating_values_df = calculate_rating_values(ratings_df)\n",
    "\n",
    "assert type(rating_values_df) == DataFrame\n",
    "assert len(rating_values_df.columns) == 5\n",
    "assert rating_values_df.count() == 83239\n",
    "\n",
    "data = rating_values_df \\\n",
    "    .filter(f.col(\"movie_id\") == \"296\") \\\n",
    "    .collect()[0]\n",
    "assert data[\"movie_id\"] == \"296\"\n",
    "assert data[\"avg_rating\"] == 4.19\n",
    "assert data[\"stddev_rating\"] == 0.95\n",
    "assert data[\"count_rating\"] == 108756\n",
    "assert data[\"min_time_rating\"] == datetime.datetime(1996, 2, 29, 10, 48, 44)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fba01-1cfc-4cad-906c-760eb205745d",
   "metadata": {},
   "source": [
    "#### Actividad 3:\n",
    "##### TO DO -> El cliente ha solicitado generar dos tablas con el mismo contenido pero con distinto esquema a partir de la tabla \"tags_df\", lee a continuación la información enviada con los requerimientos:\n",
    "- ##### En ambas tablas se requiere agrupar por \"movie_id\" y calcular:\n",
    "    - ##### El total de veces en las que aparece cada tag (en mayusculas) .\n",
    "    - ##### Fecha más antigua en la que se asignó el primer tag.\n",
    "La primer tabla deberá tener la siguiente estructura:  \n",
    "*      | movie_id |             tag_count |        min_time_tag |\n",
    "*      |        1 | [TERROR:12, SCI-FI:2] | 2017-06-02 07:20:27 |\n",
    "*      |        3 |  [DRAMA:14, SCI-FI:4] | 2012-06-02 07:20:27 |\n",
    "con esquema:\n",
    "*      |-- movie_id: string\n",
    "*      |-- tag_count: array\n",
    "*      |    |-- element: string\n",
    "*      |-- min_time_tag: timestamp\n",
    "La segunda tabla deberá tener la siguiente estructura:\n",
    "*      | movie_id |                   tag_count |        min_time_tag |\n",
    "*      |        1 | [{TERROR, 12}, {SCI-FI, 2}] | 2017-06-02 07:20:27 |\n",
    "*      |        3 |  [{DRAMA, 14}, {SCI-FI, 4}] | 2012-06-02 07:20:27 |\n",
    "con esquema:\n",
    "*      |-- movie_id: string\n",
    "*      |-- tag_count: array\n",
    "*      |    |-- element: struct\n",
    "*      |    |    |-- tag: string\n",
    "*      |    |    |-- count: long\n",
    "*      |-- min_time_tag: timestamp\n",
    "- #### La generación de la primer tabla es a través del método con la firma:\n",
    "    - ##### def get_act_3_df1(df: DataFrame) -> DataFrame\n",
    "- #### La generación de la segunda tabla es a través del método con la firma:\n",
    "    - ##### def get_act_3_df2(df: DataFrame) -> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a99bc8-bf57-47c3-95c7-4e67e013087f",
   "metadata": {},
   "source": [
    "#### La siguiente información muestra los pasos recomendados para resolver la actividad 3, puedes omitir leer esta parte si asi lo consideras.\n",
    "##### Para generar ambas tablas primero necesitamos obtener el total veces (count) en las que un \"tag\" se repite por cada \"movie_id\", para evitar conteos erroneos hay que convertir cada tag en Mayusculas.\n",
    "##### El dataframe resultante deberá contener la siguiente estructura: \"movie_id\", \"tag\", \"count\" y \"min\" donde la columna \"count\" representa el total de veces que aparecen cada \"tag\"y \"movie_id\"; la columna \"min\" representa el valor mínimo de cada \"tag\" y \"movie_id\".\n",
    "Por ejemplo, dado el dataframe de entrada\n",
    "*      |user_id|movie_id|   tag|               time|\n",
    "*      |    183|     100|sci-fi|2012-06-02 07:20:27|\n",
    "*      |     12|     832| Drama|2017-06-01 07:20:27|\n",
    "*      |    251|     100|SCI-FI|2009-06-04 07:20:27|\n",
    "*      |    265|     832| DRAMA|2015-06-08 07:20:27|\n",
    "*      |     22|     100|terror|2020-06-06 07:20:27|\n",
    "\n",
    "debemos obtener el siguiente dataframe (el nombre de columnas \"count\" y \"min\" podria ser distinto):\n",
    "\n",
    "*      | movie_id |    tag | count |                 min |\n",
    "*      |      100 | TERROR |     1 | 2020-06-06 07:20:27 |\n",
    "*      |      100 | SCI-FI |     2 | 2009-06-04 07:20:27 |\n",
    "*      |      832 |  DRAMA |     2 | 2015-06-08 07:20:27 |\n",
    "\n",
    "##### Para generar la primer tabla necesitamos concatenar las columnas \"tag\" y \"count\", posteriormente agrupando por \"movie_id\" generaremos la lista de elementos requerida. La columna \"min_time_tag\" representa el valor mínimo de cada \"movie_id\".\n",
    "- ##### Funciones de Spark recomendadas:\n",
    "    - upper -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html#pyspark.sql.functions.upper\n",
    "    - concat_ws -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat_ws.html#pyspark.sql.functions.concat_ws\n",
    "    - concat -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat.html#pyspark.sql.functions.concat\n",
    "    - collect_list -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list\n",
    "    - collect_set -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set\n",
    "##### La primer tabla quedará estructurada de la siguiente manera, donde la columna \"tag_count\" es de tipo ArrayType(StringType()) y la columna \"min_time_tag\" es de tipo TimestampType():\n",
    "Dado el dataframe de entrada\n",
    "*      | movie_id |    tag | count |                 min |\n",
    "*      |        1 | TERROR |    12 | 2020-06-06 07:20:27 |\n",
    "*      |        1 | SCI-FI |     2 | 2015-06-06 07:20:27 |\n",
    "*      |        3 |  DRAMA |    14 | 2004-06-06 07:20:27 |\n",
    "*      |        3 | SCI-FI |     4 | 2012-06-06 07:20:27 |\n",
    "debemos obtener el siguiente dataframe\n",
    "*       | movie_id |             tag_count |        min_time_tag |\n",
    "*       |        1 | [TERROR:12, SCI-FI:2] | 2015-06-06 07:20:27 |\n",
    "*       |        3 |  [DRAMA:14, SCI-FI:4] | 2004-06-06 07:20:27 |\n",
    "##### Para generar la segunda tabla necesitamos agregar en una estructura las columnas \"tag\" y \"count\", posteriormente agrupando por movie_id generaremos la lista de elementos requerida. La columna \"min_time_tag\" representa el valor mínimo de cada \"movie_id\".\n",
    "- ##### Funciones de Spark recomendadas:\n",
    "    - upper -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html#pyspark.sql.functions.upper\n",
    "    - struct -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.struct.html#pyspark.sql.functions.struct\n",
    "    - collect_list -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list\n",
    "    - collect_set -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set\n",
    "##### La segunda tabla quedará estructurada de la siguiente manera, donde la columna \"tag_count\" de tipo ArrayType(StructType(StringType(),LongType()))  y la columna \"min_time_tag\" es de tipo TimestampType():\n",
    "Dado el dataframe de entrada\n",
    "*      | movie_id |    tag | count |                 min |\n",
    "*      |        1 | TERROR |    12 | 2020-06-06 07:20:27 |\n",
    "*      |        1 | SCI-FI |     2 | 2015-06-06 07:20:27 |\n",
    "*      |        3 |  DRAMA |    14 | 2004-06-06 07:20:27 |\n",
    "*      |        3 | SCI-FI |     4 | 2012-06-06 07:20:27 |\n",
    "debemos obtener el siguiente dataframe\n",
    "*      | movie_id |                   tag_count |        min_time_tag |\n",
    "*      |        1 | [{TERROR, 12}, {SCI-FI, 2}] | 2015-06-06 07:20:27 |\n",
    "*      |        3 |  [{DRAMA, 14}, {SCI-FI, 4}] | 2004-06-06 07:20:27 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e5d81-6c6a-4e23-ad26-2da273a001e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA, PUEDES GENERAR MÉTODOS O VARIABLES NUEVAS SI ASI LO REQUIERES\n",
    "\n",
    "def get_act_3_df1(df: DataFrame):\n",
    "    return df # ... transformaciones a tags_df\n",
    "\n",
    "def get_act_3_df2(df: DataFrame):\n",
    "    return df # ... transformaciones a tags_df\n",
    "\n",
    "act_3_df1 = get_act_3_df1(tags_df)\n",
    "act_3_df2 = get_act_3_df2(tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45e9bc-cd80-4e9f-9024-613aaf1f63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "act_3_df1.show(2, False)\n",
    "act_3_df2.show(2, False)\n",
    "\"\"\"\n",
    "Ejemplo de salida esperada:\n",
    "+--------+------------------------------------------------------------------+-------------------+\n",
    "|movie_id|tag_count                                                         |min_time_rating    |\n",
    "+--------+------------------------------------------------------------------+-------------------+\n",
    "|100062  |[PRESS-GANGED : 1, WORLD WAR II : 1, WAR : 1, FATE : 1]           |2018-05-26 16:40:54|\n",
    "|100070  |[GOOD HUMOUR : 1, STRUGGLING CAREER : 1, COMEDY : 1, COMEDIAN : 2]|2017-05-19 17:17:36|\n",
    "+--------+------------------------------------------------------------------+-------------------+\n",
    "only showing top 2 rows\n",
    "\n",
    "+--------+----------------------------------------------------------------------+-------------------+\n",
    "|movie_id|tag_count                                                             |min_time_rating    |\n",
    "+--------+----------------------------------------------------------------------+-------------------+\n",
    "|100062  |[{PRESS-GANGED, 1}, {WORLD WAR II, 1}, {WAR, 1}, {FATE, 1}]           |2018-05-26 16:40:54|\n",
    "|100070  |[{GOOD HUMOUR, 1}, {STRUGGLING CAREER, 1}, {COMEDY, 1}, {COMEDIAN, 2}]|2017-05-19 17:17:36|\n",
    "+--------+----------------------------------------------------------------------+-------------------+\n",
    "only showing top 2 rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ead994-12cb-46be-a2a6-16eb5eeb9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "assert len(act_3_df1.columns) == 3\n",
    "assert \"movie_id\" in act_3_df1.columns\n",
    "assert \"tag_count\" in act_3_df1.columns\n",
    "assert \"min_time_tag\" in act_3_df1.columns\n",
    "assert schema_to_ddl(act_3_df1.select(\"movie_id\", \"tag_count\", \"min_time_tag\")) == 'movie_id STRING,tag_count ARRAY<STRING> NOT NULL,min_time_tag TIMESTAMP'\n",
    "assert act_3_df1.count() == 53452\n",
    "\n",
    "assert len(act_3_df2.columns) == 3\n",
    "assert \"movie_id\" in act_3_df2.columns\n",
    "assert \"tag_count\" in act_3_df2.columns\n",
    "assert \"min_time_tag\" in act_3_df2.columns\n",
    "assert schema_to_ddl(act_3_df2.select(\"movie_id\", \"tag_count\", \"min_time_tag\")) == 'movie_id STRING,tag_count ARRAY<STRUCT<tag: STRING, count: BIGINT>> NOT NULL,min_time_tag TIMESTAMP'\n",
    "assert act_3_df2.count() == 53452"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a5e31-597f-4117-b460-df940f85f2da",
   "metadata": {},
   "source": [
    "#### Actividad 4:\n",
    "##### TO DO -> El cliente ha solicitado que resolvamos dos consultas con la salida de la actividad 3, como no especificó con cuál dataframe requiere la consulta lo realizaremos con ambos dataframes (act_3_df1 y act_3_df2).\n",
    "##### Las consultas a resolver son:\n",
    "- ##### 1.- ¿Cuál es la pelicula (movie_id) con más tags con el valor \"SCI-FI\"?\n",
    "    - ##### Estrictamente especificó no tomar en cuenta datos como \"REALISTIC SCI-FI\", \"HARD SCI-FI\", etc.\n",
    "- ##### 2.- ¿Cuántas peliculas fueron etiquetadas como \"SCI-FI\"?\n",
    "##### La forma de resolver estas consultas será a través de un método el cual va a retornar una tupla (str, int), donde el primer elemento (str) representa el resultado de la consulta 1 y el segundo elemento (int) representa el resultado de la consulta 2\n",
    "- ##### La firma del método que utilizará cada dataFrame es:\n",
    "    - ##### def exercise_4_df1(df: DataFrame) -> firma para el dataFrame act_3_df1\n",
    "    - ##### def exercise_4_df2(df: DataFrame) -> firma para el dataFrame act_3_df2\n",
    "##### Funciones de Spark recomendadas:\n",
    "- explode -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.upper.html#pyspark.sql.functions.upper\n",
    "- regexp_extract -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_extract.html#pyspark.sql.functions.regexp_extract\n",
    "##### Funciones de la clase Column recomendadas:\n",
    "- like -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.like.html#pyspark.sql.Column.like\n",
    "- ilike -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.ilike.html#pyspark.sql.Column.ilike\n",
    "- getField -> https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.getField.html#pyspark.sql.Column.getField\n",
    "##### Acciones que podrias utilizar:\n",
    "- count, first, head, take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8701f0-c9e5-4561-bafe-97346f3552d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TU CODIGO VA EN ESTA CELDA, PUEDES GENERAR MÉTODOS O VARIABLES SI ASI LO REQUIERES\n",
    "\n",
    "def exercise_4_df1(df: DataFrame):\n",
    "    return (\"movie_id\", 0) # modificar codigo interno\n",
    "\n",
    "def exercise_4_df2(df: DataFrame):\n",
    "    return (\"movie_id\", 0) # modificar codigo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8378f9d-5823-43db-b8cf-86c3cdf99e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "print(exercise_4_df1(act_3_df1))\n",
    "print(exercise_4_df2(act_3_df2))\n",
    "# Salida esperada en ambos casos\n",
    "#('260', 854)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df8125-9055-445b-8afe-aa8d123b8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id, count = exercise_4_df1(act_3_df1)\n",
    "assert movie_id == \"260\"\n",
    "assert count == 854\n",
    "\n",
    "movie_id, count = exercise_4_df2(act_3_df2)\n",
    "assert movie_id == \"260\"\n",
    "assert count == 854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76900e-7ba9-4d99-bb20-2540cca7d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO MODIFICAR EL CONTENIDO DE ESTA CELDA\n",
    "dfs = [(movies_df, \"07/movies\"),\n",
    "       (act_3_df1, \"07/tags_p1\"),\n",
    "       (act_3_df2, \"07/tags_p2\"),\n",
    "       (calculate_rating_values(ratings_df), \"07/ratings\")]\n",
    "\n",
    "write_tmp_df(dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
